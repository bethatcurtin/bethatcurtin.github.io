{
    "openai/o3-pro": {
        "name": "OpenAI: o3 Pro",
        "id": "openai/o3-pro",
        "description": "The o-series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o3-pro model uses more compute to think harder and provide consistently better answers.  Note that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "file",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "seed",
            "max_tokens",
            "response_format",
            "structured_outputs"
        ],
        "price_prompt": "0.00002",
        "price_completion": "0.00008",
        "provider_tags": [
            "openai"
        ]
    },
    "anthropic/claude-opus-4": {
        "name": "Anthropic: Claude Opus 4",
        "id": "anthropic/claude-opus-4",
        "description": "Claude Opus 4 is benchmarked as the world’s best coding model, at time of release, bringing sustained performance on complex, long-running tasks and agent workflows. It sets new benchmarks in software engineering, achieving leading results on SWE-bench (72.5%) and Terminal-bench (43.2%). Opus 4 supports extended, agentic workflows, handling thousands of task steps continuously for hours without degradation.   Read more at the [blog post here](https://www.anthropic.com/news/claude-4)",
        "modality": "text+image->text",
        "input_modalities": [
            "image",
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "stop",
            "reasoning",
            "include_reasoning",
            "tools",
            "tool_choice",
            "top_p",
            "top_k"
        ],
        "price_prompt": "0.000015",
        "price_completion": "0.000075",
        "provider_tags": [
            "anthropic",
            "amazon-bedrock",
            "google-vertex"
        ]
    },
    "anthropic/claude-sonnet-4": {
        "name": "Anthropic: Claude Sonnet 4",
        "id": "anthropic/claude-sonnet-4",
        "description": "Claude Sonnet 4 significantly enhances the capabilities of its predecessor, Sonnet 3.7, excelling in both coding and reasoning tasks with improved precision and controllability. Achieving state-of-the-art performance on SWE-bench (72.7%), Sonnet 4 balances capability and computational efficiency, making it suitable for a broad range of applications from routine coding tasks to complex software development projects. Key enhancements include improved autonomous codebase navigation, reduced error rates in agent-driven workflows, and increased reliability in following intricate instructions. Sonnet 4 is optimized for practical everyday use, providing advanced reasoning capabilities while maintaining efficiency and responsiveness in diverse internal and external scenarios.  Read more at the [blog post here](https://www.anthropic.com/news/claude-4)",
        "modality": "text+image->text",
        "input_modalities": [
            "image",
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "stop",
            "reasoning",
            "include_reasoning",
            "tools",
            "tool_choice",
            "top_p",
            "top_k"
        ],
        "price_prompt": "0.000003",
        "price_completion": "0.000015",
        "provider_tags": [
            "google-vertex",
            "google-vertex/europe",
            "amazon-bedrock",
            "anthropic"
        ]
    },
    "openai/o3": {
        "name": "OpenAI: o3",
        "id": "openai/o3",
        "description": "o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images. Note that BYOK is required for this model. Set up here: https://openrouter.ai/settings/integrations",
        "modality": "text+image->text",
        "input_modalities": [
            "image",
            "text",
            "file"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "seed",
            "max_tokens",
            "response_format",
            "structured_outputs"
        ],
        "price_prompt": "0.000002",
        "price_completion": "0.000008",
        "provider_tags": [
            "openai"
        ]
    },
    "meta-llama/llama-4-maverick": {
        "name": "Meta: Llama 4 Maverick",
        "id": "meta-llama/llama-4-maverick",
        "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.  Maverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "top_k",
            "seed",
            "min_p",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "tools",
            "tool_choice",
            "structured_outputs"
        ],
        "price_prompt": "0.00000015",
        "price_completion": "0.0000006",
        "provider_tags": [
            "deepinfra/base",
            "parasail/fp8",
            "klusterai/fp8",
            "novita/fp8",
            "lambda/fp8",
            "baseten/fp8",
            "centml/fp8",
            "groq",
            "ncompass/fp8",
            "fireworks",
            "gmicloud/fp8",
            "together/fp8",
            "google-vertex",
            "deepinfra/turbo",
            "sambanova"
        ]
    },
    "meta-llama/llama-4-scout": {
        "name": "Meta: Llama 4 Scout",
        "id": "meta-llama/llama-4-scout",
        "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.  Built for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "seed",
            "response_format",
            "tools",
            "tool_choice",
            "structured_outputs",
            "repetition_penalty",
            "top_k",
            "top_logprobs",
            "logprobs",
            "logit_bias",
            "min_p"
        ],
        "price_prompt": "0.00000008",
        "price_completion": "0.0000003",
        "provider_tags": [
            "lambda/fp8",
            "deepinfra/bf16",
            "klusterai",
            "gmicloud/bf16",
            "parasail/fp8",
            "centml/bf16",
            "novita",
            "groq",
            "baseten/fp8",
            "fireworks",
            "together",
            "google-vertex",
            "sambanova",
            "cerebras"
        ]
    },
    "openai/o1-pro": {
        "name": "OpenAI: o1-pro",
        "id": "openai/o1-pro",
        "description": "The o1 series of models are trained with reinforcement learning to think before they answer and perform complex reasoning. The o1-pro model uses more compute to think harder and provide consistently better answers.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "seed",
            "max_tokens",
            "response_format",
            "structured_outputs"
        ],
        "price_prompt": "0.00015",
        "price_completion": "0.0006",
        "provider_tags": [
            "openai"
        ]
    },
    "meta-llama/llama-guard-3-8b": {
        "name": "Llama Guard 3 8B",
        "id": "meta-llama/llama-guard-3-8b",
        "description": "Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classification) and in LLM responses (response classification). It acts as an LLM – it generates text in its output that indicates whether a given prompt or response is safe or unsafe, and if unsafe, it also lists the content categories violated.  Llama Guard 3 was aligned to safeguard against the MLCommons standardized hazards taxonomy and designed to support Llama 3.1 capabilities. Specifically, it provides content moderation in 8 languages, and was optimized to support safety and security for search and code interpreter tool calls. ",
        "modality": "text->text",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "top_k",
            "repetition_penalty",
            "response_format",
            "structured_outputs",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "min_p",
            "seed"
        ],
        "price_prompt": "0.00000002",
        "price_completion": "0.00000006",
        "provider_tags": [
            "nebius",
            "deepinfra/bf16",
            "fireworks",
            "together",
            "groq",
            "sambanova",
            "cloudflare"
        ]
    },
    "openai/o1": {
        "name": "OpenAI: o1",
        "id": "openai/o1",
        "description": "The latest and strongest model family from OpenAI, o1 is designed to spend more time thinking before responding. The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought.   The o1 models are optimized for math, science, programming, and other STEM-related tasks. They consistently exhibit PhD-level accuracy on benchmarks in physics, chemistry, and biology. Learn more in the [launch announcement](https://openai.com/o1). ",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "seed",
            "max_tokens",
            "response_format",
            "structured_outputs"
        ],
        "price_prompt": "0.000015",
        "price_completion": "0.00006",
        "provider_tags": [
            "openai"
        ]
    },
    "amazon/nova-lite-v1": {
        "name": "Amazon: Nova Lite 1.0",
        "id": "amazon/nova-lite-v1",
        "description": "Amazon Nova Lite 1.0 is a very low-cost multimodal model from Amazon that focused on fast processing of image, video, and text inputs to generate text output. Amazon Nova Lite can handle real-time customer interactions, document analysis, and visual question-answering tasks with high accuracy.  With an input context of 300K tokens, it can analyze multiple images or up to 30 minutes of video in a single input.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.00000006",
        "price_completion": "0.00000024",
        "provider_tags": [
            "amazon-bedrock"
        ]
    },
    "amazon/nova-micro-v1": {
        "name": "Amazon: Nova Micro 1.0",
        "id": "amazon/nova-micro-v1",
        "description": "Amazon Nova Micro 1.0 is a text-only model that delivers the lowest latency responses in the Amazon Nova family of models at a very low cost. With a context length of 128K tokens and optimized for speed and cost, Amazon Nova Micro excels at tasks such as text summarization, translation, content classification, interactive chat, and brainstorming. It has  simple mathematical reasoning and coding abilities.",
        "modality": "text->text",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.000000035",
        "price_completion": "0.00000014",
        "provider_tags": [
            "amazon-bedrock"
        ]
    },
    "amazon/nova-pro-v1": {
        "name": "Amazon: Nova Pro 1.0",
        "id": "amazon/nova-pro-v1",
        "description": "Amazon Nova Pro 1.0 is a capable multimodal model from Amazon focused on providing a combination of accuracy, speed, and cost for a wide range of tasks. As of December 2024, it achieves state-of-the-art performance on key benchmarks including visual question answering (TextVQA) and video understanding (VATEX).  Amazon Nova Pro demonstrates strong capabilities in processing both visual and textual information and at analyzing financial documents.  **NOTE**: Video input is not supported at this time.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.0000008",
        "price_completion": "0.0000032",
        "provider_tags": [
            "amazon-bedrock"
        ]
    },
    "openai/chatgpt-4o-latest": {
        "name": "OpenAI: ChatGPT-4o",
        "id": "openai/chatgpt-4o-latest",
        "description": "OpenAI ChatGPT 4o is continually updated by OpenAI to point to the current version of GPT-4o used by ChatGPT. It therefore differs slightly from the API version of [GPT-4o](/models/openai/gpt-4o) in that it has additional RLHF. It is intended for research and evaluation.  OpenAI notes that this model is not suited for production use-cases as it may be removed or redirected to another model in the future.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "seed",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "response_format",
            "structured_outputs"
        ],
        "price_prompt": "0.000005",
        "price_completion": "0.000015",
        "provider_tags": [
            "openai"
        ]
    },
    "mistralai/mistral-nemo": {
        "name": "Mistral: Mistral Nemo",
        "id": "mistralai/mistral-nemo",
        "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA.  The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi.  It supports function calling and is released under the Apache 2.0 license.",
        "modality": "text->text",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "repetition_penalty",
            "response_format",
            "top_k",
            "seed",
            "min_p",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "tools",
            "tool_choice",
            "structured_outputs"
        ],
        "price_prompt": "0.00000001",
        "price_completion": "0.000000027",
        "provider_tags": [
            "klusterai",
            "deepinfra/fp8",
            "enfer/fp8",
            "parasail/fp8",
            "nextbit/bf16",
            "inference-net/fp8",
            "nebius/fp8",
            "novita",
            "inocloud/bf16",
            "mistral",
            "azure"
        ]
    },
    "openai/gpt-4o": {
        "name": "OpenAI: GPT-4o",
        "id": "openai/gpt-4o",
        "description": "GPT-4o ('o' for 'omni') is OpenAI's latest AI model, supporting both text and image inputs with text outputs. It maintains the intelligence level of [GPT-4 Turbo](/models/openai/gpt-4-turbo) while being twice as fast and 50% more cost-effective. GPT-4o also offers improved performance in processing non-English languages and enhanced visual capabilities.  For benchmarking against other models, it was briefly called ['im-also-a-good-gpt2-chatbot'](https://twitter.com/LiamFedus/status/1790064963966370209)  #multimodal",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image",
            "file"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "web_search_options",
            "seed",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "response_format",
            "structured_outputs"
        ],
        "price_prompt": "0.0000025",
        "price_completion": "0.00001",
        "provider_tags": [
            "openai",
            "azure"
        ]
    },
    "openai/gpt-4-turbo": {
        "name": "OpenAI: GPT-4 Turbo",
        "id": "openai/gpt-4-turbo",
        "description": "The latest GPT-4 Turbo model with vision capabilities. Vision requests can now use JSON mode and function calling.  Training data: up to December 2023.",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "seed",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "response_format"
        ],
        "price_prompt": "0.00001",
        "price_completion": "0.00003",
        "provider_tags": [
            "openai"
        ]
    },
    "anthropic/claude-3-haiku": {
        "name": "Anthropic: Claude 3 Haiku",
        "id": "anthropic/claude-3-haiku",
        "description": "Claude 3 Haiku is Anthropic's fastest and most compact model for near-instant responsiveness. Quick and accurate targeted performance.  See the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-haiku)  #multimodal",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.00000025",
        "price_completion": "0.00000125",
        "provider_tags": [
            "anthropic",
            "google-vertex"
        ]
    },
    "anthropic/claude-3-opus": {
        "name": "Anthropic: Claude 3 Opus",
        "id": "anthropic/claude-3-opus",
        "description": "Claude 3 Opus is Anthropic's most powerful model for highly complex tasks. It boasts top-level performance, intelligence, fluency, and understanding.  See the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)  #multimodal",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.000015",
        "price_completion": "0.000075",
        "provider_tags": [
            "anthropic",
            "google-vertex"
        ]
    },
    "anthropic/claude-3-sonnet": {
        "name": "Anthropic: Claude 3 Sonnet",
        "id": "anthropic/claude-3-sonnet",
        "description": "Claude 3 Sonnet is an ideal balance of intelligence and speed for enterprise workloads. Maximum utility at a lower price, dependable, balanced for scaled deployments.  See the launch announcement and benchmark results [here](https://www.anthropic.com/news/claude-3-family)  #multimodal",
        "modality": "text+image->text",
        "input_modalities": [
            "text",
            "image"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.000003",
        "price_completion": "0.000015",
        "provider_tags": [
            "anthropic",
            "google-vertex"
        ]
    },
    "mistralai/mistral-large": {
        "name": "Mistral Large",
        "id": "mistralai/mistral-large",
        "description": "This is Mistral AI's flagship model, Mistral Large 2 (version `mistral-large-2407`). It's a proprietary weights-available model and excels at reasoning, code, JSON, chat, and more. Read the launch announcement [here](https://mistral.ai/news/mistral-large-2407/).  It supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Its long context window allows precise information recall from large documents.",
        "modality": "text->text",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "response_format",
            "stop",
            "seed",
            "frequency_penalty",
            "presence_penalty",
            "structured_outputs"
        ],
        "price_prompt": "0.000002",
        "price_completion": "0.000006",
        "provider_tags": [
            "mistral",
            "azure"
        ]
    },
    "anthropic/claude-2": {
        "name": "Anthropic: Claude v2",
        "id": "anthropic/claude-2",
        "description": "Claude 2 delivers advancements in key capabilities for enterprises—including an industry-leading 200K token context window, significant reductions in rates of model hallucination, system prompts and a new beta feature: tool use.",
        "modality": "text->text",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "max_tokens",
            "temperature",
            "top_p",
            "top_k",
            "stop"
        ],
        "price_prompt": "0.000008",
        "price_completion": "0.000024",
        "provider_tags": [
            "anthropic"
        ]
    },
    "openai/gpt-4": {
        "name": "OpenAI: GPT-4",
        "id": "openai/gpt-4",
        "description": "OpenAI's flagship model, GPT-4 is a large-scale multimodal language model capable of solving difficult problems with greater accuracy than previous models due to its broader general knowledge and advanced reasoning capabilities. Training data: up to Sep 2021.",
        "modality": "text->text",
        "input_modalities": [
            "text"
        ],
        "output_modalities": [
            "text"
        ],
        "supported_parameters": [
            "tools",
            "tool_choice",
            "max_tokens",
            "temperature",
            "top_p",
            "stop",
            "frequency_penalty",
            "presence_penalty",
            "seed",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "response_format"
        ],
        "price_prompt": "0.00003",
        "price_completion": "0.00006",
        "provider_tags": [
            "openai",
            "azure"
        ]
    },
    "models_list": [
        "amazon/nova-lite-v1",
        "amazon/nova-micro-v1",
        "amazon/nova-pro-v1",
        "anthropic/claude-2",
        "anthropic/claude-3-haiku",
        "anthropic/claude-3-opus",
        "anthropic/claude-3-sonnet",
        "anthropic/claude-sonnet-4",
        "mistralai/mistral-nemo",
        "openai/chatgpt-4o-latest",
        "openai/gpt-4",
        "openai/gpt-4-turbo",
        "openai/gpt-4o"
    ]
}
